nohup: ignoring input
Loaded model from checkpoints/pretrained.pt
Split sizes - Total: 1072302, Train: 857841, Val: 107230, Test: 107231
Training on 857841 examples, validating on 107230 examples
Using stratified val subset of 495 samples
=== RL Epoch 1/10 ===
Step 100: avg_reward=1.2750, loss=-4.9704
Step 200: avg_reward=1.4954, loss=-2.2945
Step 300: avg_reward=1.2725, loss=-3.4211
Step 400: avg_reward=1.1761, loss=-4.6488
Step 500: avg_reward=1.3369, loss=-3.2313
Step 500 [EVAL]: train_reward=1.3807, val_reward=1.3232, val_optimal=53.33%, val_suboptimal=19.19%
Saved to checkpoints/rl_finetuned.pt
Step 600: avg_reward=1.5492, loss=-2.5070
Step 700: avg_reward=1.1925, loss=-2.2303
Step 800: avg_reward=1.3929, loss=-2.4522
Step 900: avg_reward=1.6762, loss=-2.3097
Step 1000: avg_reward=1.3400, loss=-3.0274
Step 1000 [EVAL]: train_reward=1.4030, val_reward=1.3406, val_optimal=53.74%, val_suboptimal=19.60%
Saved to checkpoints/rl_finetuned.pt
Step 1100: avg_reward=1.3546, loss=-4.5171
Step 1200: avg_reward=1.1782, loss=-4.0360
Step 1300: avg_reward=1.3271, loss=-2.4036
Step 1400: avg_reward=1.5007, loss=-2.6016
Step 1500: avg_reward=1.5055, loss=-2.7102
Step 1500 [EVAL]: train_reward=1.4130, val_reward=1.3948, val_optimal=53.33%, val_suboptimal=23.23%
Saved to checkpoints/rl_finetuned.pt
Step 1600: avg_reward=1.2218, loss=-3.3225
Step 1700: avg_reward=1.4164, loss=-2.2065
Step 1800: avg_reward=1.2085, loss=-3.2335
Step 1900: avg_reward=1.5067, loss=-1.7851
Step 2000: avg_reward=1.3953, loss=-2.9576
Step 2000 [EVAL]: train_reward=1.4211, val_reward=1.3904, val_optimal=54.55%, val_suboptimal=21.62%
No improvement. Patience: 1/5
Step 2100: avg_reward=1.4317, loss=-2.4323
Step 2200: avg_reward=1.5451, loss=-1.9499
Step 2300: avg_reward=1.3771, loss=-3.9595
Step 2400: avg_reward=1.2412, loss=-3.0849
Step 2500: avg_reward=1.5843, loss=-1.7674
Step 2500 [EVAL]: train_reward=1.4329, val_reward=1.3782, val_optimal=53.54%, val_suboptimal=22.02%
No improvement. Patience: 2/5
Step 2600: avg_reward=1.1364, loss=-3.3194
Step 2700: avg_reward=1.4299, loss=-2.5893
Step 2800: avg_reward=1.5637, loss=-1.3657
Step 2900: avg_reward=1.3489, loss=-2.9037
Step 3000: avg_reward=1.3531, loss=-2.1050
Step 3000 [EVAL]: train_reward=1.4323, val_reward=1.3851, val_optimal=53.94%, val_suboptimal=22.42%
No improvement. Patience: 3/5
Step 3100: avg_reward=1.3031, loss=-3.5036
Step 3200: avg_reward=1.3979, loss=-3.1763
Step 3300: avg_reward=1.4706, loss=-3.1611
Step 3400: avg_reward=1.2507, loss=-2.1257
Step 3500: avg_reward=1.4728, loss=-1.4110
Step 3500 [EVAL]: train_reward=1.4437, val_reward=1.3885, val_optimal=54.55%, val_suboptimal=22.42%
No improvement. Patience: 4/5
Step 3600: avg_reward=1.3162, loss=-3.1381
Step 3700: avg_reward=1.3553, loss=-4.3350
Step 3800: avg_reward=1.4473, loss=-2.2948
Step 3900: avg_reward=1.4276, loss=-3.2335
Step 4000: avg_reward=1.4192, loss=-2.4024
Step 4000 [EVAL]: train_reward=1.4390, val_reward=1.4026, val_optimal=54.14%, val_suboptimal=22.83%
No improvement. Patience: 5/5
Early stopping at step 4000
Done. Checkpoint saved to checkpoints/rl_finetuned.pt
